{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4ca1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets \n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n",
    "from thop import profile\n",
    "from netcal.metrics import ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d34df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43bee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baixando de https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz...\n",
      "Extraindo...\n",
      "Extração concluída em data\\flowers-102\n",
      "Baixando labels...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169M/169M [00:13<00:00, 12.6MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todos os datasets estão prontos com train, val e test!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "import scipy.io\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def download_and_extract_tgz(url, download_path, extract_path):\n",
    "    if not os.path.exists(download_path):\n",
    "        print(f\"Baixando de {url}...\")\n",
    "        urllib.request.urlretrieve(url, download_path)\n",
    "    else:\n",
    "        print(f\"{download_path} já existe, pulando download.\")\n",
    "\n",
    "    if not os.path.exists(os.path.join(extract_path, \"jpg\")):\n",
    "        print(\"Extraindo...\")\n",
    "        with tarfile.open(download_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_path)\n",
    "        print(f\"Extração concluída em {extract_path}\")\n",
    "    else:\n",
    "        print(\"Pasta 'jpg' já existe, pulando extração.\")\n",
    "\n",
    "def split_dataset(dataset, val_frac=0.1, seed=42):\n",
    "    n_val = int(len(dataset) * val_frac)\n",
    "    n_train = len(dataset) - n_val\n",
    "    torch.manual_seed(seed)\n",
    "    return random_split(dataset, [n_train, n_val])\n",
    "\n",
    "def prepare_flowers102(data_root=\"data\", seed=42):\n",
    "    random.seed(seed)\n",
    "    flowers_path = os.path.join(data_root, \"flowers-102\")\n",
    "    os.makedirs(flowers_path, exist_ok=True)\n",
    "\n",
    "    # Baixar imagens\n",
    "    url_flowers = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
    "    tgz_path = os.path.join(flowers_path, \"102flowers.tgz\")\n",
    "    img_folder = os.path.join(flowers_path, \"jpg\")\n",
    "\n",
    "    if not os.path.exists(img_folder):\n",
    "        download_and_extract_tgz(url_flowers, tgz_path, flowers_path)\n",
    "\n",
    "    url_labels = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n",
    "    labels_path = os.path.join(flowers_path, \"imagelabels.mat\")\n",
    "    if not os.path.exists(labels_path):\n",
    "        print(\"Baixando labels...\")\n",
    "        urllib.request.urlretrieve(url_labels, labels_path)\n",
    "\n",
    "    labels = scipy.io.loadmat(labels_path)['labels'][0]  # array 1D com valores 1 a 102\n",
    "    all_images = sorted(os.listdir(img_folder))\n",
    "\n",
    "    n_total = len(all_images)\n",
    "    n_train = int(0.7 * n_total)\n",
    "    n_val = int(0.15 * n_total)\n",
    "    n_test = n_total - n_train - n_val\n",
    "\n",
    "    splits = {\n",
    "        \"train\": all_images[:n_train],\n",
    "        \"val\": all_images[n_train:n_train+n_val],\n",
    "        \"test\": all_images[n_train+n_val:]\n",
    "    }\n",
    "\n",
    "    for split_name, imgs in splits.items():\n",
    "        split_dir = os.path.join(flowers_path, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "        for img_name in imgs:\n",
    "            idx = int(img_name[6:11]) - 1 \n",
    "            label = str(labels[idx])\n",
    "            label_dir = os.path.join(split_dir, label)\n",
    "            os.makedirs(label_dir, exist_ok=True)\n",
    "            shutil.copy(os.path.join(img_folder, img_name), os.path.join(label_dir, img_name))\n",
    "\n",
    "    return os.path.join(flowers_path, \"train\"), os.path.join(flowers_path, \"val\"), os.path.join(flowers_path, \"test\")\n",
    "\n",
    "\n",
    "def load_datasets(batch_size=64, seed=42):\n",
    "    data_root = \"data\"\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    train_dir, val_dir, test_dir = prepare_flowers102(data_root, seed)\n",
    "    train_flowers = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "    val_flowers   = datasets.ImageFolder(val_dir, transform=transform_test)\n",
    "    test_flowers  = datasets.ImageFolder(test_dir, transform=transform_test)\n",
    "\n",
    "    tiny_path = os.path.join(data_root, \"tiny-imagenet-200\")\n",
    "    if not os.path.exists(tiny_path):\n",
    "        url_tiny = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "        zip_path = os.path.join(data_root, \"tiny-imagenet-200.zip\")\n",
    "        urllib.request.urlretrieve(url_tiny, zip_path)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_root)\n",
    "        os.remove(zip_path)\n",
    "\n",
    "    train_tiny = datasets.ImageFolder(os.path.join(tiny_path, 'train'), transform=transform_train)\n",
    "    val_tiny, test_tiny = split_dataset(\n",
    "        datasets.ImageFolder(os.path.join(tiny_path, 'val'), transform=transform_test), val_frac=0.5, seed=seed\n",
    "    )\n",
    "    train_tiny, val_tiny = split_dataset(train_tiny, val_frac=0.1, seed=seed)\n",
    "\n",
    "    train_cifar_full = datasets.CIFAR100(root=data_root, train=True, transform=transform_train, download=True)\n",
    "    train_cifar, val_cifar = split_dataset(train_cifar_full, val_frac=0.1, seed=seed)\n",
    "    test_cifar = datasets.CIFAR100(root=data_root, train=False, transform=transform_test, download=True)\n",
    "\n",
    "    train_loaders = {\n",
    "        \"Flowers-102\": DataLoader(train_flowers, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        \"Tiny-ImageNet\": DataLoader(train_tiny, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        \"CIFAR-100\": DataLoader(train_cifar, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "    }\n",
    "\n",
    "    val_loaders = {\n",
    "        \"Flowers-102\": DataLoader(val_flowers, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"Tiny-ImageNet\": DataLoader(val_tiny, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"CIFAR-100\": DataLoader(val_cifar, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "    }\n",
    "\n",
    "    test_loaders = {\n",
    "        \"Flowers-102\": DataLoader(test_flowers, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"Tiny-ImageNet\": DataLoader(test_tiny, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"CIFAR-100\": DataLoader(test_cifar, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "    }\n",
    "\n",
    "    return train_loaders, val_loaders, test_loaders\n",
    "if __name__ == \"__main__\":\n",
    "    train_loaders, val_loaders, test_loaders = load_datasets()\n",
    "    print(\"Todos os datasets estão prontos com train, val e test!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
