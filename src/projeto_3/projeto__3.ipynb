{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4ca1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets \n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n",
    "from thop import profile\n",
    "from netcal.metrics import ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d34df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c43bee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Divisão estratificada concluída!\n",
      "✅ Todos os datasets estão prontos com train, val e test!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "import scipy.io\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "def download_and_extract_tgz(url, download_path, extract_path):\n",
    "    if not os.path.exists(download_path):\n",
    "        print(f\"Baixando de {url}...\")\n",
    "        urllib.request.urlretrieve(url, download_path)\n",
    "    else:\n",
    "        print(f\"{download_path} já existe, pulando download.\")\n",
    "\n",
    "    if not os.path.exists(os.path.join(extract_path, \"jpg\")):\n",
    "        print(\"Extraindo...\")\n",
    "        with tarfile.open(download_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_path)\n",
    "        print(f\"Extração concluída em {extract_path}\")\n",
    "    else:\n",
    "        print(\"Pasta 'jpg' já existe, pulando extração.\")\n",
    "\n",
    "def split_dataset(dataset, val_frac=0.1, seed=42):\n",
    "    n_val = int(len(dataset) * val_frac)\n",
    "    n_train = len(dataset) - n_val\n",
    "    torch.manual_seed(seed)\n",
    "    return random_split(dataset, [n_train, n_val])\n",
    "\n",
    "def prepare_flowers102(data_root=\"data\", seed=42):\n",
    "    import numpy as np\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    flowers_path = os.path.join(data_root, \"flowers-102\")\n",
    "    os.makedirs(flowers_path, exist_ok=True)\n",
    "\n",
    "    # Baixar imagens\n",
    "    url_flowers = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
    "    tgz_path = os.path.join(flowers_path, \"102flowers.tgz\")\n",
    "    img_folder = os.path.join(flowers_path, \"jpg\")\n",
    "\n",
    "    if not os.path.exists(img_folder):\n",
    "        download_and_extract_tgz(url_flowers, tgz_path, flowers_path)\n",
    "\n",
    "    # Baixar labels\n",
    "    url_labels = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n",
    "    labels_path = os.path.join(flowers_path, \"imagelabels.mat\")\n",
    "    if not os.path.exists(labels_path):\n",
    "        print(\"Baixando labels...\")\n",
    "        urllib.request.urlretrieve(url_labels, labels_path)\n",
    "\n",
    "    labels = scipy.io.loadmat(labels_path)['labels'][0]  # array 1D com valores 1 a 102\n",
    "    all_images = sorted(os.listdir(img_folder))\n",
    "\n",
    "    # Dicionário: classe -> lista de imagens\n",
    "    class_to_images = {i: [] for i in range(1, 103)}\n",
    "    for img_name in all_images:\n",
    "        idx = int(img_name[6:11]) - 1  # ex: image_00001.jpg → índice 0\n",
    "        label = int(labels[idx])\n",
    "        class_to_images[label].append(img_name)\n",
    "\n",
    "    # Criar pastas destino\n",
    "    for split_name in [\"train\", \"val\", \"test\"]:\n",
    "        split_dir = os.path.join(flowers_path, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "    # Divisão estratificada\n",
    "    for cls, imgs in class_to_images.items():\n",
    "        random.shuffle(imgs)\n",
    "        n_total = len(imgs)\n",
    "        n_train = int(0.7 * n_total)\n",
    "        n_val = int(0.15 * n_total)\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        splits = {\n",
    "            \"train\": imgs[:n_train],\n",
    "            \"val\": imgs[n_train:n_train + n_val],\n",
    "            \"test\": imgs[n_train + n_val:]\n",
    "        }\n",
    "\n",
    "        for split_name, img_list in splits.items():\n",
    "            label_dir = os.path.join(flowers_path, split_name, str(cls))\n",
    "            os.makedirs(label_dir, exist_ok=True)\n",
    "            for img_name in img_list:\n",
    "                src = os.path.join(img_folder, img_name)\n",
    "                dst = os.path.join(label_dir, img_name)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy(src, dst)\n",
    "\n",
    "    print(\"✅ Divisão estratificada concluída!\")\n",
    "    return (\n",
    "        os.path.join(flowers_path, \"train\"),\n",
    "        os.path.join(flowers_path, \"val\"),\n",
    "        os.path.join(flowers_path, \"test\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def load_datasets(batch_size=64, seed=42):\n",
    "    data_root = \"data\"\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "    # ✅ Normalização adicionada (padrão ImageNet)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    # Flowers-102\n",
    "    train_dir, val_dir, test_dir = prepare_flowers102(data_root, seed)\n",
    "    train_flowers = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "    val_flowers   = datasets.ImageFolder(val_dir, transform=transform_test)\n",
    "    test_flowers  = datasets.ImageFolder(test_dir, transform=transform_test)\n",
    "\n",
    "    # Tiny-ImageNet\n",
    "    tiny_path = os.path.join(data_root, \"tiny-imagenet-200\")\n",
    "    if not os.path.exists(tiny_path):\n",
    "        url_tiny = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "        zip_path = os.path.join(data_root, \"tiny-imagenet-200.zip\")\n",
    "        urllib.request.urlretrieve(url_tiny, zip_path)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_root)\n",
    "        os.remove(zip_path)\n",
    "\n",
    "    train_tiny = datasets.ImageFolder(os.path.join(tiny_path, 'train'), transform=transform_train)\n",
    "    val_tiny, test_tiny = split_dataset(\n",
    "        datasets.ImageFolder(os.path.join(tiny_path, 'val'), transform=transform_test), val_frac=0.5, seed=seed\n",
    "    )\n",
    "    train_tiny, val_tiny = split_dataset(train_tiny, val_frac=0.1, seed=seed)\n",
    "\n",
    "    # CIFAR-100\n",
    "    train_cifar_full = datasets.CIFAR100(root=data_root, train=True, transform=transform_train, download=True)\n",
    "    train_cifar, val_cifar = split_dataset(train_cifar_full, val_frac=0.1, seed=seed)\n",
    "    test_cifar = datasets.CIFAR100(root=data_root, train=False, transform=transform_test, download=True)\n",
    "\n",
    "    # DataLoaders\n",
    "    train_loaders = {\n",
    "        \"Flowers-102\": DataLoader(train_flowers, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        \"Tiny-ImageNet\": DataLoader(train_tiny, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        \"CIFAR-100\": DataLoader(train_cifar, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "    }\n",
    "\n",
    "    val_loaders = {\n",
    "        \"Flowers-102\": DataLoader(val_flowers, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"Tiny-ImageNet\": DataLoader(val_tiny, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"CIFAR-100\": DataLoader(val_cifar, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "    }\n",
    "\n",
    "    test_loaders = {\n",
    "        \"Flowers-102\": DataLoader(test_flowers, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"Tiny-ImageNet\": DataLoader(test_tiny, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"CIFAR-100\": DataLoader(test_cifar, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "    }\n",
    "\n",
    "    return train_loaders, val_loaders, test_loaders\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loaders, val_loaders, test_loaders = load_datasets()\n",
    "    print(\"✅ Todos os datasets estão prontos com train, val e test!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "091b746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Subset\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, c, r=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(c, c // r, 1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c // r, c, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.fc(x)\n",
    "        return x * w\n",
    "\n",
    "class VGG_Autoral(nn.Module):\n",
    "    def __init__(self, num_classes=1000, dropout=0.5):\n",
    "        super(VGG_Autoral, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SEBlock(256),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for dataset_name, test_loader in test_loaders.items():\n",
    "    dataset = train_loaders[dataset_name].dataset\n",
    "    \n",
    "    if hasattr(dataset, \"dataset\"):\n",
    "        dataset = dataset.dataset\n",
    "\n",
    "    num_classes = len(dataset.classes)\n",
    "\n",
    "    model = VGG_Autoral(num_classes=num_classes)\n",
    "    model.to(device)\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "158d06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from thop import profile\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_ece(probs, labels, n_bins=15):\n",
    "        confidences = np.max(probs, axis=1)       \n",
    "        predictions = np.argmax(probs, axis=1)    \n",
    "        accuracies = predictions == labels\n",
    "\n",
    "        bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        bin_indices = np.digitize(confidences, bins) - 1\n",
    "\n",
    "        ece = 0.0\n",
    "        for i in range(n_bins):\n",
    "            mask = bin_indices == i\n",
    "            if np.any(mask):\n",
    "                bin_acc = np.mean(accuracies[mask])\n",
    "                bin_conf = np.mean(confidences[mask])\n",
    "                ece += np.abs(bin_acc - bin_conf) * np.mean(mask)\n",
    "        return ece\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_model(model, dataloader, device='cuda'):\n",
    "        model.to(device) \n",
    "        model.eval()\n",
    "        all_preds, all_labels, all_probs = [], [], []\n",
    "        total_loss = 0.0\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():  # <-- Corrigido aqui\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                preds = np.argmax(probs, axis=1)\n",
    "\n",
    "                all_probs.append(probs)\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        ece = Metrics.compute_ece(all_probs, all_labels)\n",
    "\n",
    "        return {\n",
    "            \"Acurácia\": acc,\n",
    "            \"Precisão\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1\": f1,\n",
    "            \"Matriz_Confusão\": cm,\n",
    "            \"ECE\": ece,\n",
    "            \"Loss_Média\": total_loss / len(dataloader.dataset)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model_complexity(model, input_size=(1, 3, 224, 224)):\n",
    "        input_tensor = torch.randn(*input_size)\n",
    "        device = next(model.parameters()).device\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        flops, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "        return {\"FLOPs\": flops, \"Parâmetros\": params}\n",
    "\n",
    "    @staticmethod\n",
    "    def benchmark_model(model, device='cuda', input_size=(1, 3, 224, 224), runs=50):\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        x = torch.randn(input_size).to(device)\n",
    "\n",
    "        # Warm-up\n",
    "        for _ in range(10):\n",
    "            _ = model(x)\n",
    "\n",
    "        torch.cuda.synchronize() if device == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "\n",
    "        for _ in range(runs):\n",
    "            _ = model(x)\n",
    "\n",
    "        torch.cuda.synchronize() if device == 'cuda' else None\n",
    "        elapsed = (time.time() - start_time) / runs\n",
    "\n",
    "        if device == 'cuda':\n",
    "            GPU = GPUtil.getGPUs()[0]\n",
    "            mem_used = GPU.memoryUsed\n",
    "        else:\n",
    "            mem_used = psutil.virtual_memory().used / (1024 ** 3)\n",
    "\n",
    "        return {\"Latência (s)\": elapsed, \"Memória (MB)\": mem_used}\n",
    "\n",
    "    @staticmethod\n",
    "    def comparar_modelos(modelos, dataloader, device='cuda'):\n",
    "        resultados = {}\n",
    "        for nome, modelo in modelos.items():\n",
    "            modelo.to(device)\n",
    "            metricas = Metrics.evaluate_model(modelo, dataloader, device)\n",
    "            complexidade = Metrics.get_model_complexity(modelo)\n",
    "            desempenho = Metrics.benchmark_model(modelo, device)\n",
    "            resultados[nome] = {**metricas, **complexidade, **desempenho}\n",
    "        return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586fd48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "def top_confused_classes(cm, class_names, top_k=10):\n",
    "    \"\"\"\n",
    "    Retorna as top_k classes mais confundidas e a submatriz correspondente.\n",
    "    \"\"\"\n",
    "    # Zerar a diagonal (acertos)\n",
    "    cm_errors = cm.copy()\n",
    "    np.fill_diagonal(cm_errors, 0)\n",
    "\n",
    "    # Pegar os indices dos top_k maiores erros\n",
    "    flat_indices = np.argsort(cm_errors.flatten())[-top_k:]\n",
    "    rows, cols = np.unravel_index(flat_indices, cm_errors.shape)\n",
    "\n",
    "    # Classes únicas envolvidas nos top_k erros\n",
    "    selected_classes = np.unique(np.concatenate([rows, cols]))\n",
    "    selected_cm = cm[np.ix_(selected_classes, selected_classes)]\n",
    "    selected_class_names = [class_names[i] for i in selected_classes]\n",
    "\n",
    "    return selected_cm, selected_class_names\n",
    "\n",
    "def plot_top_confusion(cm, class_names, top_k=10, model_name=\"Modelo\"):\n",
    "    selected_cm, selected_class_names = top_confused_classes(cm, class_names, top_k)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(selected_cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=selected_class_names, yticklabels=selected_class_names)\n",
    "    plt.xlabel(\"Predito\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.title(f\"Matriz de Confusão - Top {top_k} Confusões - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_metric_comparison(resultados, metric_name):\n",
    "    nomes = list(resultados.keys())\n",
    "    valores = [resultados[m][metric_name] for m in nomes]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(nomes, valores, color=['#0077b6', '#00b4d8', '#90e0ef'])\n",
    "    plt.title(f\"Comparação de {metric_name}\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aef265b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import gc, time\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device='cuda',\n",
    "                optimizer_name=\"AdamW\", lr=0.001, momentum=0.9, weight_decay=1e-4,\n",
    "                scheduler_type=\"cosine\", epochs=50, patience=10,\n",
    "                initialize=False): \n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer_name.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_name.lower() == \"adamw\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    if scheduler_type.lower() == \"cosine\":\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    elif scheduler_type.lower() == \"step\":\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "    if initialize:\n",
    "        initialize_weights(model)  \n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    early_counter = 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss = total_loss / total\n",
    "        train_acc = correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss /= val_total\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            early_counter = 0\n",
    "        else:\n",
    "            early_counter += 1\n",
    "            if early_counter >= patience:\n",
    "                print(f\"Early stopping ativado na época {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    return model, history\n",
    "\n",
    "def build_vgg_pretrained(variant, weights, num_classes):\n",
    "    model = variant(weights=weights)\n",
    "    model.classifier[6] = nn.Linear(4096, num_classes)\n",
    "    return model\n",
    "\n",
    "def profile_model(model, device='cuda', input_size=(1,3,224,224)):\n",
    "    model.to(device)\n",
    "    x = torch.randn(input_size).to(device)\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        _ = model(x)\n",
    "    torch.cuda.synchronize()\n",
    "    elapsed = time.time() - start_time\n",
    "    mem_peak = torch.cuda.max_memory_allocated() / (1024**2)\n",
    "    return {\"Tempo_Inferência_s\": elapsed, \"Pico_Memória_MB\": mem_peak}\n",
    "\n",
    "def run_experimento(modelos, train_loaders, val_loaders, device='cuda', epochs=50, patience=10):\n",
    "    resultados = {}\n",
    "\n",
    "    for dataset_name, train_loader in train_loaders.items():\n",
    "        print(f\"\\n===== Dataset: {dataset_name} =====\")\n",
    "        val_loader = val_loaders[dataset_name]\n",
    "\n",
    "        dataset_ref = train_loader.dataset\n",
    "        if hasattr(dataset_ref, \"dataset\"):\n",
    "            dataset_ref = dataset_ref.dataset\n",
    "        num_classes = len(dataset_ref.classes)\n",
    "\n",
    "        for nome, modelo in modelos.items():\n",
    "            print(f\"\\nTreinando {nome} no dataset {dataset_name}\")\n",
    "\n",
    "            if nome.startswith(\"VGG\") and nome != \"VGG_Autoral\":\n",
    "                variant = getattr(models, nome.lower())\n",
    "                pretrained_weights = getattr(models, f\"{nome}_Weights\").IMAGENET1K_V1\n",
    "                modelo_copy = build_vgg_pretrained(variant, weights=pretrained_weights, num_classes=num_classes)\n",
    "                initialize_flag = False  \n",
    "            else:\n",
    "                modelo_copy = type(modelo)(num_classes=num_classes)\n",
    "                initialize_flag = True  \n",
    "\n",
    "            modelo_copy, hist = train_model(\n",
    "                modelo_copy,\n",
    "                train_loader,\n",
    "                val_loader,\n",
    "                device=device,\n",
    "                epochs=epochs,\n",
    "                patience=patience,\n",
    "                initialize=initialize_flag\n",
    "            )\n",
    "\n",
    "            # Profiling\n",
    "            prof = profile_model(modelo_copy, device=device)\n",
    "            resultados[(dataset_name, nome)] = {**prof, **hist}\n",
    "\n",
    "            del modelo_copy\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "\n",
    "    return resultados\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf92d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "✅ Divisão estratificada concluída!\n",
      "\n",
      "===== Treinando e avaliando dataset: Flowers-102 =====\n",
      "\n",
      "=== Iniciando treinamento ===\n",
      "\n",
      "===== Dataset: Flowers-102 =====\n",
      "\n",
      "Treinando VGG_Autoral no dataset Flowers-102\n",
      "Epoch [1/70] | Train Loss: 24.0564, Train Acc: 0.0211 | Val Loss: 4.5545, Val Acc: 0.0557\n",
      "Epoch [2/70] | Train Loss: 4.5312, Train Acc: 0.0350 | Val Loss: 4.4239, Val Acc: 0.0624\n",
      "Epoch [3/70] | Train Loss: 4.4670, Train Acc: 0.0387 | Val Loss: 4.3478, Val Acc: 0.0599\n",
      "Epoch [4/70] | Train Loss: 4.4106, Train Acc: 0.0427 | Val Loss: 4.2207, Val Acc: 0.0650\n",
      "Epoch [5/70] | Train Loss: 4.3557, Train Acc: 0.0468 | Val Loss: 4.2040, Val Acc: 0.0819\n",
      "Epoch [6/70] | Train Loss: 4.3240, Train Acc: 0.0436 | Val Loss: 4.1421, Val Acc: 0.0540\n",
      "Epoch [7/70] | Train Loss: 4.2773, Train Acc: 0.0459 | Val Loss: 4.0648, Val Acc: 0.0743\n",
      "Epoch [8/70] | Train Loss: 4.2498, Train Acc: 0.0482 | Val Loss: 4.0343, Val Acc: 0.0734\n",
      "Epoch [9/70] | Train Loss: 4.2308, Train Acc: 0.0494 | Val Loss: 3.9980, Val Acc: 0.0726\n",
      "Epoch [10/70] | Train Loss: 4.2006, Train Acc: 0.0561 | Val Loss: 3.9672, Val Acc: 0.0709\n",
      "Epoch [11/70] | Train Loss: 4.1867, Train Acc: 0.0524 | Val Loss: 3.9567, Val Acc: 0.0785\n",
      "Epoch [12/70] | Train Loss: 4.1493, Train Acc: 0.0550 | Val Loss: 3.9298, Val Acc: 0.0819\n",
      "Epoch [13/70] | Train Loss: 4.1267, Train Acc: 0.0608 | Val Loss: 3.9078, Val Acc: 0.0827\n",
      "Epoch [14/70] | Train Loss: 4.1300, Train Acc: 0.0619 | Val Loss: 3.8887, Val Acc: 0.0827\n",
      "Epoch [15/70] | Train Loss: 4.0966, Train Acc: 0.0591 | Val Loss: 3.8603, Val Acc: 0.0869\n",
      "Epoch [16/70] | Train Loss: 4.0584, Train Acc: 0.0642 | Val Loss: 3.8318, Val Acc: 0.0920\n",
      "Epoch [17/70] | Train Loss: 4.0518, Train Acc: 0.0610 | Val Loss: 3.8002, Val Acc: 0.1046\n",
      "Epoch [18/70] | Train Loss: 4.0332, Train Acc: 0.0652 | Val Loss: 3.7775, Val Acc: 0.0979\n",
      "Epoch [19/70] | Train Loss: 3.9944, Train Acc: 0.0723 | Val Loss: 3.7399, Val Acc: 0.1063\n"
     ]
    }
   ],
   "source": [
    "@staticmethod\n",
    "def build_vgg_pretrained(variant, weights, num_classes):\n",
    "    model = variant(weights=weights)  \n",
    "    model.classifier[6] = nn.Linear(4096, num_classes)  \n",
    "    return model\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    from torchvision import models\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gc\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    # Carregar datasets\n",
    "    train_loaders, val_loaders, test_loaders = load_datasets()\n",
    "    resultados_totais = {}\n",
    "\n",
    "    # Função para plot de comparação\n",
    "    def plot_metric_comparison(resultados, metric_name):\n",
    "        nomes = list(resultados.keys())\n",
    "        valores = [resultados[m][metric_name] for m in nomes]\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(nomes, valores, color=['#0077b6', '#00b4d8', '#90e0ef'])\n",
    "        plt.title(f\"Comparação de {metric_name}\")\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "    for dataset_name in train_loaders.keys():\n",
    "        print(f\"\\n===== Treinando e avaliando dataset: {dataset_name} =====\")\n",
    "        train_loader = train_loaders[dataset_name]\n",
    "        val_loader = val_loaders[dataset_name]\n",
    "        test_loader = test_loaders[dataset_name]\n",
    "\n",
    "        dataset_ref = train_loader.dataset\n",
    "        if hasattr(dataset_ref, \"dataset\"):\n",
    "            dataset_ref = dataset_ref.dataset\n",
    "        num_classes = len(dataset_ref.classes)\n",
    "\n",
    "        # Criar modelos\n",
    "        modelos = {\n",
    "            #\"VGG11\": build_vgg_pretrained(models.vgg11, models.VGG11_Weights.IMAGENET1K_V1, num_classes),\n",
    "            #\"VGG13\": build_vgg_pretrained(models.vgg13, models.VGG13_Weights.IMAGENET1K_V1, num_classes),\n",
    "            #\"VGG16\": build_vgg_pretrained(models.vgg16, models.VGG16_Weights.IMAGENET1K_V1, num_classes),\n",
    "           # \"VGG19\": build_vgg_pretrained(models.vgg19, models.VGG19_Weights.IMAGENET1K_V1, num_classes),\n",
    "            \"VGG_Autoral\": VGG_Autoral(num_classes=num_classes),\n",
    "        }\n",
    "\n",
    "        for model in modelos.values():\n",
    "            model.to(device)\n",
    "            \n",
    "        # Treinamento\n",
    "        print(\"\\n=== Iniciando treinamento ===\")\n",
    "        resultados_treino = run_experimento(\n",
    "            modelos=modelos,\n",
    "            train_loaders={dataset_name: train_loader},\n",
    "            val_loaders={dataset_name: val_loader},\n",
    "            device=device,\n",
    "            epochs=70,\n",
    "            patience=10\n",
    "        )\n",
    "\n",
    "        # Avaliação completa pós-treino\n",
    "        for model_name, model in modelos.items():\n",
    "            print(f\"\\n--- Avaliando modelo: {model_name} ---\")\n",
    "            \n",
    "            # Avaliar no dataset de teste\n",
    "            metricas = Metrics.evaluate_model(model, test_loader, device=device)\n",
    "            complexidade = Metrics.get_model_complexity(model)\n",
    "            desempenho = Metrics.benchmark_model(model, device=device)\n",
    "\n",
    "            # Armazenar resultados\n",
    "            resultados_totais[(dataset_name, model_name)] = {\n",
    "                \"Teste\": metricas,\n",
    "                \"Treino\": resultados_treino.get((dataset_name, model_name), {}),\n",
    "                \"Complexidade\": complexidade,\n",
    "                \"Desempenho\": desempenho\n",
    "            }\n",
    "\n",
    "            # Mostrar métricas de teste\n",
    "            print(\"\\nMétricas de teste:\")\n",
    "            for k, v in metricas.items():\n",
    "                if isinstance(v, float):\n",
    "                    print(f\"{k}: {v:.4f}\")\n",
    "                else:\n",
    "                    print(f\"{k}: {v}\")\n",
    "\n",
    "            # Mostrar métricas de treino\n",
    "            print(\"\\nMétricas de treino:\")\n",
    "            for k, v in resultados_treino.get((dataset_name, model_name), {}).items():\n",
    "                if isinstance(v, float):\n",
    "                    print(f\"{k}: {v:.4f}\")\n",
    "                else:\n",
    "                    print(f\"{k}: {v}\")\n",
    "\n",
    "            # Plotar matriz de confusão\n",
    "            plot_top_confusion(\n",
    "                metricas['Matriz_Confusão'],\n",
    "                class_names=train_loaders[dataset_name].dataset.classes,\n",
    "                top_k=10,  # número de confusões a exibir\n",
    "                model_name=model_name\n",
    "            )\n",
    "\n",
    "        # Comparação de acurácia entre modelos\n",
    "        resultados_acuracia = {name: resultados_totais[(dataset_name, name)][\"Teste\"] for name in modelos.keys()}\n",
    "        plot_metric_comparison(resultados_acuracia, \"Acurácia\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695e48b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5070 Ti\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c5e6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
