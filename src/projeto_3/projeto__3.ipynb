{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4ca1796",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import  matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets \n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import time\n",
    "from thop import profile\n",
    "from netcal.metrics import ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d34df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c43bee16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Todos os datasets estão prontos com train, val e test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import zipfile\n",
    "import scipy.io\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data import Subset\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_num_classes(dataset):\n",
    "    while isinstance(dataset, torch.utils.data.Subset):\n",
    "        dataset = dataset.dataset\n",
    "\n",
    "    if hasattr(dataset, \"classes\"):\n",
    "        return len(dataset.classes)\n",
    "    else:\n",
    "        raise AttributeError(\"O dataset base não possui atributo 'classes'.\")\n",
    "\n",
    "\n",
    "def stratified_subsample(dataset, samples_per_class=50, seed=42):\n",
    "    random.seed(seed)\n",
    "    targets = [label for _, label in dataset.samples]\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, label in enumerate(targets):\n",
    "        class_indices[label].append(idx)\n",
    "\n",
    "    selected_indices = []\n",
    "    for label, indices in class_indices.items():\n",
    "        n = min(samples_per_class, len(indices))\n",
    "        selected_indices.extend(random.sample(indices, n))\n",
    "\n",
    "    return Subset(dataset, selected_indices)\n",
    "\n",
    "def download_and_extract_tgz(url, download_path, extract_path):\n",
    "    if not os.path.exists(download_path):\n",
    "        print(f\"Baixando em {url} \")\n",
    "        urllib.request.urlretrieve(url, download_path)\n",
    "    else:\n",
    "        print(f\"{download_path} já existe\")\n",
    "\n",
    "    if not os.path.exists(os.path.join(extract_path, \"jpg\")):\n",
    "        print(\"Extraindo...\")\n",
    "        with tarfile.open(download_path, \"r:gz\") as tar:\n",
    "            tar.extractall(path=extract_path)\n",
    "        print(f\"Extração concluída em {extract_path}\")\n",
    "    else:\n",
    "        print(\"Pasta 'jpg' já existe, pulando extração.\")\n",
    "\n",
    "def split_dataset(dataset, val_frac=0.1, seed=42):\n",
    "    n_val = int(len(dataset) * val_frac)\n",
    "    n_train = len(dataset) - n_val\n",
    "    torch.manual_seed(seed)\n",
    "    return random_split(dataset, [n_train, n_val])\n",
    "\n",
    "def prepare_flowers102(data_root=\"data\", seed=42):\n",
    "    import numpy as np\n",
    "\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    flowers_path = os.path.join(data_root, \"flowers-102\")\n",
    "    os.makedirs(flowers_path, exist_ok=True)\n",
    "\n",
    "    url_flowers = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
    "    tgz_path = os.path.join(flowers_path, \"102flowers.tgz\")\n",
    "    img_folder = os.path.join(flowers_path, \"jpg\")\n",
    "\n",
    "    if not os.path.exists(img_folder):\n",
    "        download_and_extract_tgz(url_flowers, tgz_path, flowers_path)\n",
    "\n",
    "    url_labels = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n",
    "    labels_path = os.path.join(flowers_path, \"imagelabels.mat\")\n",
    "    if not os.path.exists(labels_path):\n",
    "        print(\"Baixando labels...\")\n",
    "        urllib.request.urlretrieve(url_labels, labels_path)\n",
    "\n",
    "    labels = scipy.io.loadmat(labels_path)['labels'][0]\n",
    "    all_images = sorted(os.listdir(img_folder))\n",
    "\n",
    "    class_to_images = {i: [] for i in range(1, 103)}\n",
    "    for img_name in all_images:\n",
    "        idx = int(img_name[6:11]) - 1\n",
    "        label = int(labels[idx])\n",
    "        class_to_images[label].append(img_name)\n",
    "\n",
    "    for split_name in [\"train\", \"val\", \"test\"]:\n",
    "        split_dir = os.path.join(flowers_path, split_name)\n",
    "        os.makedirs(split_dir, exist_ok=True)\n",
    "\n",
    "    for cls, imgs in class_to_images.items():\n",
    "        random.shuffle(imgs)\n",
    "        n_total = len(imgs)\n",
    "        n_train = int(0.7 * n_total)\n",
    "        n_val = int(0.15 * n_total)\n",
    "        n_test = n_total - n_train - n_val\n",
    "\n",
    "        splits = {\n",
    "            \"train\": imgs[:n_train],\n",
    "            \"val\": imgs[n_train:n_train + n_val],\n",
    "            \"test\": imgs[n_train + n_val:]\n",
    "        }\n",
    "\n",
    "        for split_name, img_list in splits.items():\n",
    "            label_dir = os.path.join(flowers_path, split_name, str(cls))\n",
    "            os.makedirs(label_dir, exist_ok=True)\n",
    "            for img_name in img_list:\n",
    "                src = os.path.join(img_folder, img_name)\n",
    "                dst = os.path.join(label_dir, img_name)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.copy(src, dst)\n",
    "\n",
    "    return (\n",
    "        os.path.join(flowers_path, \"train\"),\n",
    "        os.path.join(flowers_path, \"val\"),\n",
    "        os.path.join(flowers_path, \"test\")\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def load_datasets(batch_size=64, seed=42):\n",
    "    data_root = \"data\"\n",
    "    os.makedirs(data_root, exist_ok=True)\n",
    "\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])\n",
    "\n",
    "    train_dir, val_dir, test_dir = prepare_flowers102(data_root, seed)\n",
    "    train_flowers = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "    val_flowers   = datasets.ImageFolder(val_dir, transform=transform_test)\n",
    "    test_flowers  = datasets.ImageFolder(test_dir, transform=transform_test)\n",
    "\n",
    "    tiny_path = os.path.join(data_root, \"tiny-imagenet-200\")\n",
    "    if not os.path.exists(tiny_path):\n",
    "        url_tiny = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\"\n",
    "        zip_path = os.path.join(data_root, \"tiny-imagenet-200.zip\")\n",
    "        urllib.request.urlretrieve(url_tiny, zip_path)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(data_root)\n",
    "        os.remove(zip_path)\n",
    "\n",
    "    train_tiny_full = datasets.ImageFolder(os.path.join(tiny_path, 'train'), transform=transform_train)\n",
    "    val_tiny_full = datasets.ImageFolder(os.path.join(tiny_path, 'val'), transform=transform_test)\n",
    "\n",
    "    train_tiny_full = stratified_subsample(train_tiny_full, samples_per_class=150, seed=seed)\n",
    "    val_tiny_full = stratified_subsample(val_tiny_full, samples_per_class=30, seed=seed)\n",
    "\n",
    "    train_tiny, val_extra = split_dataset(train_tiny_full, val_frac=0.1, seed=seed)\n",
    "\n",
    "    test_tiny = val_tiny_full\n",
    "\n",
    "\n",
    "    train_cifar_full = datasets.CIFAR100(root=data_root, train=True, transform=transform_train, download=True)\n",
    "    train_cifar, val_cifar = split_dataset(train_cifar_full, val_frac=0.1, seed=seed)\n",
    "    test_cifar = datasets.CIFAR100(root=data_root, train=False, transform=transform_test, download=True)\n",
    "\n",
    "    train_loaders = {\n",
    "        \n",
    "        \"Tiny-ImageNet\": DataLoader(train_tiny, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        \"CIFAR-100\": DataLoader(train_cifar, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "        \"Flowers-102\": DataLoader(train_flowers, batch_size=batch_size, shuffle=True, num_workers=2),\n",
    "    }\n",
    "\n",
    "    val_loaders = {\n",
    "        \n",
    "        \"Tiny-ImageNet\": DataLoader(val_extra, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"CIFAR-100\": DataLoader(val_cifar, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"Flowers-102\": DataLoader(val_flowers, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "    }\n",
    "\n",
    "    test_loaders = {\n",
    "        \n",
    "        \"Tiny-ImageNet\": DataLoader(test_tiny, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"CIFAR-100\": DataLoader(test_cifar, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "        \"Flowers-102\": DataLoader(test_flowers, batch_size=batch_size, shuffle=False, num_workers=2),\n",
    "    }\n",
    "\n",
    "    return train_loaders, val_loaders, test_loaders\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_loaders, val_loaders, test_loaders = load_datasets()\n",
    "    print(\" Todos os datasets estão prontos com train, val e test\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "091b746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, c, r=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(c, c // r, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(c // r, c, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class FireX(nn.Module):\n",
    "    def __init__(self, in_channels, squeeze_channels, expand1x1_channels, expand3x3_channels, use_se=False, dw_expand=False):\n",
    "        super(FireX, self).__init__()\n",
    "        self.use_se = use_se\n",
    "        self.squeeze = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, squeeze_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(squeeze_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        if dw_expand:\n",
    "            self.expand1x1 = DepthwiseSeparableConv(squeeze_channels, expand1x1_channels, kernel_size=1, padding=0)\n",
    "            self.expand3x3 = DepthwiseSeparableConv(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1)\n",
    "        else:\n",
    "            self.expand1x1 = nn.Sequential(\n",
    "                nn.Conv2d(squeeze_channels, expand1x1_channels, kernel_size=1),\n",
    "                nn.BatchNorm2d(expand1x1_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "            self.expand3x3 = nn.Sequential(\n",
    "                nn.Conv2d(squeeze_channels, expand3x3_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm2d(expand3x3_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            )\n",
    "        out_channels = expand1x1_channels + expand3x3_channels\n",
    "        self.se = SEBlock(out_channels) if use_se else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.squeeze(x)\n",
    "        x1 = self.expand1x1(x)\n",
    "        x3 = self.expand3x3(x)\n",
    "        h = min(x1.shape[2], x3.shape[2])\n",
    "        w = min(x1.shape[3], x3.shape[3])\n",
    "        x1 = x1[:, :, :h, :w]\n",
    "        x3 = x3[:, :, :h, :w]\n",
    "        out = torch.cat([x1, x3], dim=1)\n",
    "        return self.se(out)\n",
    "\n",
    "\n",
    "class SqueezeNetAutoral(nn.Module):\n",
    "    def __init__(self, num_classes=100, use_se=True, dw_expand=False, small=True):\n",
    "        super(SqueezeNetAutoral, self).__init__()\n",
    "        if small:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(32),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(3, stride=2, padding=1),\n",
    "                FireX(32, 8, 32, 32, use_se=use_se, dw_expand=dw_expand),\n",
    "                FireX(64, 16, 48, 48, use_se=use_se, dw_expand=dw_expand),\n",
    "                nn.MaxPool2d(3, stride=2, padding=1),\n",
    "                FireX(96, 24, 64, 64, use_se=use_se, dw_expand=dw_expand),\n",
    "                FireX(128, 24, 64, 64, use_se=use_se, dw_expand=dw_expand),\n",
    "                nn.MaxPool2d(3, stride=2, padding=1),\n",
    "                FireX(128, 32, 96, 96, use_se=use_se, dw_expand=dw_expand),\n",
    "                FireX(192, 48, 128, 128, use_se=use_se, dw_expand=dw_expand)\n",
    "            )\n",
    "            final_planes = 256\n",
    "        else:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(64),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(3, stride=2, padding=1),\n",
    "                FireX(64, 16, 64, 64, use_se=use_se, dw_expand=dw_expand),\n",
    "                FireX(128, 16, 64, 64, use_se=use_se, dw_expand=dw_expand),\n",
    "                nn.MaxPool2d(3, stride=2, padding=1),\n",
    "                FireX(128, 32, 128, 128, use_se=use_se, dw_expand=dw_expand),\n",
    "                FireX(256, 32, 128, 128, use_se=use_se, dw_expand=dw_expand),\n",
    "                nn.MaxPool2d(3, stride=2, padding=1),\n",
    "                FireX(256, 48, 192, 192, use_se=use_se, dw_expand=dw_expand),\n",
    "                FireX(384, 64, 256, 256, use_se=use_se, dw_expand=dw_expand)\n",
    "            )\n",
    "            final_planes = 512\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(final_planes, num_classes)\n",
    "        )\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if getattr(m, \"bias\", None) is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "158d06e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from thop import profile\n",
    "import psutil\n",
    "import GPUtil\n",
    "\n",
    "class Metrics:\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_ece(probs, labels, n_bins=15):\n",
    "        confidences = np.max(probs, axis=1)       \n",
    "        predictions = np.argmax(probs, axis=1)    \n",
    "        accuracies = predictions == labels\n",
    "\n",
    "        bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "        bin_indices = np.digitize(confidences, bins) - 1\n",
    "\n",
    "        ece = 0.0\n",
    "        for i in range(n_bins):\n",
    "            mask = bin_indices == i\n",
    "            if np.any(mask):\n",
    "                bin_acc = np.mean(accuracies[mask])\n",
    "                bin_conf = np.mean(confidences[mask])\n",
    "                ece += np.abs(bin_acc - bin_conf) * np.mean(mask)\n",
    "        return ece\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate_model(model, dataloader, device='cuda'):\n",
    "        model.to(device) \n",
    "        model.eval()\n",
    "        all_preds, all_labels, all_probs = [], [], []\n",
    "        total_loss = 0.0\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        with torch.no_grad():  \n",
    "            for inputs, labels in dataloader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                preds = np.argmax(probs, axis=1)\n",
    "\n",
    "                all_probs.append(probs)\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        all_probs = np.concatenate(all_probs)\n",
    "        all_preds = np.concatenate(all_preds)\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        prec = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        rec = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1 = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        cm = confusion_matrix(all_labels, all_preds)\n",
    "        ece = Metrics.compute_ece(all_probs, all_labels)\n",
    "\n",
    "        return {\n",
    "            \"Acurácia\": acc,\n",
    "            \"Precisão\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1\": f1,\n",
    "            \"Matriz_Confusão\": cm,\n",
    "            \"ECE\": ece,\n",
    "            \"Loss_Média\": total_loss / len(dataloader.dataset)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_model_complexity(model, input_size=(1, 3, 224, 224)):\n",
    "        input_tensor = torch.randn(*input_size)\n",
    "        device = next(model.parameters()).device\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        flops, params = profile(model, inputs=(input_tensor,), verbose=False)\n",
    "        return {\"FLOPs\": flops, \"Parâmetros\": params}\n",
    "\n",
    "    @staticmethod\n",
    "    def benchmark_model(model, device='cuda', input_size=(1, 3, 224, 224), runs=50):\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        x = torch.randn(input_size).to(device)\n",
    "\n",
    "        for _ in range(10):\n",
    "            _ = model(x)\n",
    "\n",
    "        torch.cuda.synchronize() if device == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "\n",
    "        for _ in range(runs):\n",
    "            _ = model(x)\n",
    "\n",
    "        torch.cuda.synchronize() if device == 'cuda' else None\n",
    "        elapsed = (time.time() - start_time) / runs\n",
    "\n",
    "        if device == 'cuda':\n",
    "            GPU = GPUtil.getGPUs()[0]\n",
    "            mem_used = GPU.memoryUsed\n",
    "        else:\n",
    "            mem_used = psutil.virtual_memory().used / (1024 ** 3)\n",
    "\n",
    "        return {\"Latência (s)\": elapsed, \"Memória (MB)\": mem_used}\n",
    "\n",
    "    @staticmethod\n",
    "    def comparar_modelos(modelos, dataloader, device='cuda'):\n",
    "        resultados = {}\n",
    "        for nome, modelo in modelos.items():\n",
    "            modelo.to(device)\n",
    "            metricas = Metrics.evaluate_model(modelo, dataloader, device)\n",
    "            complexidade = Metrics.get_model_complexity(modelo)\n",
    "            desempenho = Metrics.benchmark_model(modelo, device)\n",
    "            resultados[nome] = {**metricas, **complexidade, **desempenho}\n",
    "        return resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586fd48a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def top_confused_classes(cm, class_names, top_k=10):\n",
    "    cm_errors = cm.copy()\n",
    "    np.fill_diagonal(cm_errors, 0)\n",
    "\n",
    "    flat_indices = np.argsort(cm_errors.flatten())[-top_k:]\n",
    "    rows, cols = np.unravel_index(flat_indices, cm_errors.shape)\n",
    "\n",
    "    selected_classes = np.unique(np.concatenate([rows, cols]))\n",
    "    selected_cm = cm[np.ix_(selected_classes, selected_classes)]\n",
    "    selected_class_names = [class_names[i] for i in selected_classes]\n",
    "\n",
    "    return selected_cm, selected_class_names\n",
    "\n",
    "def plot_top_confusion(cm, class_names, top_k=10, model_name=\"Modelo\"):\n",
    "    selected_cm, selected_class_names = top_confused_classes(cm, class_names, top_k)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(selected_cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=selected_class_names, yticklabels=selected_class_names)\n",
    "    plt.xlabel(\"Predito\")\n",
    "    plt.ylabel(\"Real\")\n",
    "    plt.title(f\"Matriz de Confusão - Top {top_k} Confusões - {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_metric_comparison(resultados, metric_name):\n",
    "    nomes = list(resultados.keys())\n",
    "    valores = [resultados[m][metric_name] for m in nomes]\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(nomes, valores, color=['#0077b6', '#00b4d8', '#90e0ef'])\n",
    "    plt.title(f\"Comparação de {metric_name}\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aef265b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "import gc, time\n",
    "import copy\n",
    "\n",
    "def initialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            if m.weight is not None:\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            if m.weight is not None:\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            if m.weight is not None:\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device='cuda',\n",
    "                optimizer_name=\"AdamW\", lr=0.001, momentum=0.9, weight_decay=1e-4,\n",
    "                scheduler_type=\"cosine\", epochs=50, patience=10, initialize=False):\n",
    "\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    if optimizer_name.lower() == \"sgd\":\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "    elif optimizer_name.lower() == \"adamw\":\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    scheduler = (\n",
    "        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "        if scheduler_type.lower() == \"cosine\"\n",
    "        else optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    )\n",
    "\n",
    "    if initialize:\n",
    "        initialize_weights(model)\n",
    "\n",
    "    best_val_acc, best_epoch = 0.0, 0\n",
    "    best_model_wts, early_counter = None, 0\n",
    "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0.0, 0, 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            correct += (outputs.argmax(1) == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        train_loss, train_acc = total_loss / total, correct / total\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct, val_total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_correct += (outputs.argmax(1) == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss, val_acc = val_loss / val_total, val_correct / val_total\n",
    "        scheduler.step()\n",
    "\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] | Train Acc: {train_acc:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc, best_epoch = val_acc, epoch + 1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            early_counter = 0\n",
    "        else:\n",
    "            early_counter += 1\n",
    "            if early_counter >= patience:\n",
    "                print(f\"Early stopping na época {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    if best_model_wts is not None:\n",
    "        model.load_state_dict(best_model_wts)\n",
    "        print(f\" Melhor modelo: época {best_epoch}, Val Acc = {best_val_acc:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cf92d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "\n",
      "===== Treinando e avaliando dataset: Tiny-ImageNet =====\n",
      "\n",
      "=== Iniciando treinamento por 50 épocas ===\n",
      "\n",
      " Treinando SqueezeNet1_0...\n",
      "\n",
      "Epoch [1/50] | Train Acc: 0.0101 | Val Acc: 0.0113\n",
      "Epoch [2/50] | Train Acc: 0.0127 | Val Acc: 0.0217\n",
      "Epoch [3/50] | Train Acc: 0.0177 | Val Acc: 0.0200\n",
      "Epoch [4/50] | Train Acc: 0.0221 | Val Acc: 0.0213\n",
      "Epoch [5/50] | Train Acc: 0.0245 | Val Acc: 0.0293\n",
      "Epoch [6/50] | Train Acc: 0.0274 | Val Acc: 0.0293\n",
      "Epoch [7/50] | Train Acc: 0.0321 | Val Acc: 0.0300\n",
      "Epoch [8/50] | Train Acc: 0.0354 | Val Acc: 0.0400\n",
      "Epoch [9/50] | Train Acc: 0.0426 | Val Acc: 0.0480\n"
     ]
    }
   ],
   "source": [
    "def get_classes_from_loader(loader):\n",
    "    dataset = loader.dataset\n",
    "    while hasattr(dataset, \"dataset\"):\n",
    "        dataset = dataset.dataset\n",
    "    return dataset.classes\n",
    "\n",
    "def run_experimento(modelos, train_loader, val_loader, device, epochs=50, patience=10):\n",
    "    resultados_treino = {}\n",
    "    modelos_treinados = {}\n",
    "    for nome, modelo in modelos.items():\n",
    "        print(f\"\\n Treinando {nome}...\\n\")\n",
    "        modelo_treinado, historico = train_model(\n",
    "            modelo,\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            device=device,\n",
    "            optimizer_name=\"AdamW\",\n",
    "            lr=1e-4,\n",
    "            epochs=epochs,\n",
    "            patience=patience\n",
    "        )\n",
    "        resultados_treino[nome] = historico\n",
    "        modelos_treinados[nome] = modelo_treinado\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return resultados_treino, modelos_treinados\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import torch\n",
    "    from torchvision import models\n",
    "    import torch.nn as nn\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gc\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "    train_loaders, val_loaders, test_loaders = load_datasets()\n",
    "    resultados_totais = {}\n",
    "\n",
    "    def plot_metric_comparison(resultados, metric_name):\n",
    "        nomes = list(resultados.keys())\n",
    "        valores = [resultados[m][metric_name] for m in nomes]\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(nomes, valores, color=['#0077b6', '#00b4d8', '#90e0ef'])\n",
    "        plt.title(f\"Comparação de {metric_name}\")\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "        plt.show()\n",
    "\n",
    "    for dataset_name in train_loaders.keys():\n",
    "        print(f\"\\n===== Treinando e avaliando dataset: {dataset_name} =====\")\n",
    "        train_loader = train_loaders[dataset_name]\n",
    "        val_loader = val_loaders[dataset_name]\n",
    "        test_loader = test_loaders[dataset_name]\n",
    "        num_classes = get_num_classes(train_loader.dataset)\n",
    "\n",
    "        modelos = {\n",
    "            \"SqueezeNet1_0\": models.squeezenet1_0(weights=None),\n",
    "            \"SqueezeNet_Autoral\": SqueezeNetAutoral(num_classes=num_classes, use_se=True, dw_expand=True, small=True),\n",
    "            \"SqueezeNet1_1\": models.squeezenet1_1(weights=None)\n",
    "        }\n",
    "\n",
    "        # Ajusta os classifiers e inicializa pesos\n",
    "        for nome, modelo in modelos.items():\n",
    "            if \"Autoral\" not in nome:\n",
    "                # Ajusta primeira camada para imagens pequenas\n",
    "                modelo.features[0] = nn.Conv2d(3, 96, kernel_size=3, stride=1, padding=1)\n",
    "                \n",
    "                # Ajusta o classifier para o número de classes\n",
    "                modelo.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1, 1))\n",
    "                modelo.num_classes = num_classes\n",
    "                \n",
    "                # Inicializa pesos\n",
    "                initialize_weights(modelo)\n",
    "\n",
    "            modelo.to(device)\n",
    "\n",
    "\n",
    "        # Define epochs dependendo do dataset\n",
    "        if dataset_name in [\"CIFAR-100\", \"Tiny-ImageNet\"]:\n",
    "            epochs = 50\n",
    "        else:\n",
    "            epochs = 100  \n",
    "\n",
    "        print(f\"\\n=== Iniciando treinamento por {epochs} épocas ===\")\n",
    "        resultados_treino, modelos_treinados = run_experimento(\n",
    "            modelos=modelos,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            epochs=epochs,\n",
    "            patience=12\n",
    "        )\n",
    "\n",
    "        for model_name in modelos.keys():\n",
    "            print(f\"\\n--- Avaliando modelo: {model_name} ---\")\n",
    "            model_treinado = modelos_treinados[model_name]\n",
    "            metricas = Metrics.evaluate_model(model_treinado, test_loader, device=device)\n",
    "            complexidade = Metrics.get_model_complexity(model_treinado)\n",
    "            desempenho = Metrics.benchmark_model(model_treinado, device=device)\n",
    "            resultados_totais[(dataset_name, model_name)] = {\n",
    "                \"Teste\": metricas,\n",
    "                \"Treino\": resultados_treino[model_name],\n",
    "                \"Complexidade\": complexidade,\n",
    "                \"Desempenho\": desempenho\n",
    "            }\n",
    "            for k, v in metricas.items():\n",
    "                print(f\"{k}: {v:.4f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "            plot_top_confusion(\n",
    "                metricas['Matriz_Confusão'],\n",
    "                class_names=get_classes_from_loader(train_loader),\n",
    "                top_k=10,\n",
    "                model_name=model_name\n",
    "            )\n",
    "\n",
    "        resultados_acuracia = {name: resultados_totais[(dataset_name, name)][\"Teste\"] for name in modelos.keys()}\n",
    "        plot_metric_comparison(resultados_acuracia, \"Acurácia\")\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "695e48b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 5070 Ti\n",
      "12.8\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.version.cuda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c5e6e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
